1.XGBoost 和 GBDT 的区别？他们在缺失值处理之间的差别？
答案：算法层面的：
XGB加了正则项，普通GBDT没有。为了防止过拟合
xgboost损失函数是误差部分是二阶泰勒展开，GBDT 是一阶泰勒展开。因此损失函数近似的更精准。
对每颗子树增加一个参数，使得每颗子树的权重降低，防止过拟合，这个参数叫shrinkage
对特征进行降采样，灵感来源于随机森林，除了能降低计算量外，还能防止过拟合。
实现了利用分捅/分位数方法，实现了全局和局部的近似分裂点算法，降低了计算量，并且在eps参数设置合理的情况下，能达到穷举法几乎一样的性能
提出并实现了特征带权重的分位数的方法
增加处理缺失值的方案（通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向）。
系统层面：
对每个特征进行分块（block）并排序，使得在寻找最佳分裂点的时候能够并行化计算。这是xgboost比一般GBDT更快的一个重要原因。
通过设置合理的block的大小，充分利用了CPU缓存进行读取加速（cache-aware access）。使得数据读取的速度更快。因为太小的block的尺寸使得多线程中每个线程负载太小降低了并行效率。太大的block尺寸会导致CPU的缓存获取miss掉。
out-of-core 通过将block压缩（block compressoin）并存储到硬盘上，并且通过将block分区到多个硬盘上（block Sharding）实现了更大的IO 读写速度，因此，因为加入了硬盘存储block读写的部分不仅仅使得xgboost处理大数据量的能力有所提升，并且通过提高IO的吞吐量使得xgboost相比一般实利用这种技术实现大数据计算的框架更快。

2.XGBoost和random forest的不同
答案：样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
预测函数：
Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
https://www.cnblogs.com/earendil/p/8872001.html

3.了解朴素贝叶斯吗？
答案：朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法。朴素贝叶斯算法是假设各个特征之间相互独立。
https://zhuanlan.zhihu.com/p/26262151

4.你在项目中常用的特征选择方法有哪些？
答案：去掉取值变化小的特征
单变量特征选择
线性模型和正则化
https://blog.csdn.net/akenseren/article/details/80816210

5.实际应用中冷启动的问题你怎么解决？
答案：基于有代表性的内容。

6.说一说有监督学习和无监督学习的区别
答案:有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。
有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。
非监督学习方法在寻找数据集中的规律性，并不需要划分数据集。
https://blog.csdn.net/u010947534/article/details/82025794

7.如何处理缺失值
答案：均值插补:如果样本属性的距离是可度量的，则使用该属性有效值的平均值来插补缺失的值；如果的距离是不可度量的，则使用该属性有效值的众数来插补缺失的值。
同类均值插补:首先将样本进行分类，然后以该类中样本的均值来插补缺失值。
建模预测:将缺失的属性作为预测目标来预测，将数据集按照是否含有特定属性的缺失值分为两类，利用现有的机器学习算法对待预测数据集的缺失值进行预测。
如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。
https://blog.csdn.net/ningyanggege/article/details/88428880
